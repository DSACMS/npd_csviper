#!/usr/bin/env python3
"""
MySQL CSV Import Script - Generated by CSViper

This script imports CSV data into a MySQL database using pre-generated SQL scripts.

Original CSV: test_data.csv
Generated on: 2025-06-23 02:19:48
"""

import os
import sys
import json
import csv
import click
from dotenv import load_dotenv


def find_post_import_sql_files(script_dir):
    """
    Find and return post-import SQL files in execution order.
    
    Args:
        script_dir (str): Directory containing the script
        
    Returns:
        List[Tuple[int, str]]: List of (order, filepath) tuples sorted by order
    """
    import glob
    
    post_import_dir = os.path.join(script_dir, 'post_import_sql')
    if not os.path.exists(post_import_dir):
        return []
    
    files_with_order = []
    
    # Look for SQL files in subdirectories
    for root, dirs, files in os.walk(post_import_dir):
        for filename in files:
            # First, look for MySQL-specific files
            if filename.endswith('.mysql.sql'):
                # Extract numeric prefix
                try:
                    order_str = filename.split('_')[0]
                    order = int(order_str)
                    filepath = os.path.join(root, filename)
                    files_with_order.append((order, filepath))
                except (ValueError, IndexError):
                    # Skip files that don't follow the naming convention
                    continue
    
    # If no MySQL-specific files found, look for generic .sql files
    if not files_with_order:
        for root, dirs, files in os.walk(post_import_dir):
            for filename in files:
                if filename.endswith('.sql') and not filename.endswith('.postgresql.sql'):
                    # Extract numeric prefix
                    try:
                        order_str = filename.split('_')[0]
                        order = int(order_str)
                        filepath = os.path.join(root, filename)
                        files_with_order.append((order, filepath))
                    except (ValueError, IndexError):
                        # Skip files that don't follow the naming convention
                        continue
    
    # Sort by order
    files_with_order.sort(key=lambda x: x[0])
    
    return files_with_order


def execute_post_import_sql(connection, post_import_files, db_schema_name, table_name):
    """
    Execute post-import SQL files in order.
    
    Args:
        connection: Database connection
        post_import_files (List[Tuple[int, str]]): List of (order, filepath) tuples
        db_schema_name (str): Database schema name
        table_name (str): Table name
    """
    if not post_import_files:
        click.echo("No post-import SQL files found")
        return
    
    click.echo(f"Executing {len(post_import_files)} post-import SQL files...")
    
    with connection.cursor() as cursor:
        for order, filepath in post_import_files:
            filename = os.path.basename(filepath)
            click.echo(f"Executing post-import SQL: {filename}")
            
            # Read SQL file
            with open(filepath, 'r', encoding='utf-8') as f:
                sql_content = f.read()
            
            # Replace placeholders
            sql_content = sql_content.replace('REPLACE_ME_DATABASE_NAME', db_schema_name) \
                                   .replace('REPLACE_ME_TABLE_NAME', table_name)
            
            # Split into individual statements and execute
            statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]
            for statement in statements:
                if statement and not statement.startswith('--'):
                    try:
                        click.echo(f"  Executing: {statement}")
                        cursor.execute(statement)
                        connection.commit()
                    except Exception as e:
                        click.echo(f"Warning: Error executing statement in {filename}: {e}")
                        click.echo(f"  Failed statement: {statement}")
                        # Continue with next statement
                        continue
    
    click.echo("✓ Post-import SQL execution completed")


def find_env_file():
    """
    Find .env file in current directory or parent directory.
    
    Returns:
        str: Path to .env file or None if not found
    """
    # Check current directory
    current_dir_env = os.path.join(os.getcwd(), '.env')
    if os.path.exists(current_dir_env):
        return current_dir_env
    
    # Check parent directory
    parent_dir_env = os.path.join(os.path.dirname(os.getcwd()), '.env')
    if os.path.exists(parent_dir_env):
        return parent_dir_env
    
    return None


def check_gitignore_for_env():
    """
    Check if .env is excluded in local .gitignore file.
    Warns if .env is not properly excluded.
    """
    gitignore_path = os.path.join(os.getcwd(), '.gitignore')
    if os.path.exists(gitignore_path):
        with open(gitignore_path, 'r') as f:
            gitignore_content = f.read()
            if '.env' not in gitignore_content:
                click.echo("Warning: .env file should be added to .gitignore to avoid committing credentials")
    else:
        click.echo("Warning: No .gitignore file found. Consider creating one and adding .env to it")


def validate_csv_header(csv_file, expected_columns):
    """
    Validate that CSV header matches expected columns from metadata.
    
    Args:
        csv_file (str): Path to CSV file
        expected_columns (list): Expected column names from metadata
        
    Raises:
        ValueError: If headers don't match
    """
    with open(csv_file, 'r', newline='') as f:
        reader = csv.reader(f)
        actual_header = next(reader)
    
    if len(actual_header) != len(expected_columns):
        raise ValueError(f"Column count mismatch: Expected {len(expected_columns)}, got {len(actual_header)}")
    
    for i, (expected, actual) in enumerate(zip(expected_columns, actual_header)):
        if expected != actual:
            raise ValueError(f"Column {i+1} mismatch: Expected '{expected}', got '{actual}'")


def load_sql_file(filename):
    """
    Load SQL content from file.
    
    Args:
        filename (str): SQL filename
        
    Returns:
        str: SQL content
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    sql_path = os.path.join(script_dir, filename)
    
    if not os.path.exists(sql_path):
        raise FileNotFoundError(f"SQL file not found: {sql_path}")
    
    with open(sql_path, 'r') as f:
        return f.read()


def replace_sql_placeholders(sql_content, db_name, table_name, csv_path):
    """
    Replace placeholders in SQL content with actual values.
    
    Args:
        sql_content (str): SQL content with placeholders
        db_name (str): Database/schema name
        table_name (str): Table name
        csv_path (str): Full path to CSV file
        
    Returns:
        str: SQL content with placeholders replaced
    """
    return sql_content.replace('REPLACE_ME_DB_NAME', db_name) \
                     .replace('REPLACE_ME_TABLE_NAME', table_name) \
                     .replace('REPLACE_ME_CSV_FULL_PATH', csv_path)


def execute_mysql_import(db_config, db_schema_name, table_name, csv_file, trample):
    """
    Execute MySQL import process.
    
    Args:
        db_config (dict): Database configuration
        db_schema_name (str): Database schema name
        table_name (str): Table name
        csv_file (str): Path to CSV file
        trample (bool): Whether to overwrite existing data
    """
    try:
        import pymysql
    except ImportError:
        raise ImportError("pymysql library is required for MySQL imports. Install with: pip install pymysql")
    
    # Load SQL files
    create_table_sql = load_sql_file('test_data.create_table_mysql.sql')
    import_data_sql = load_sql_file('test_data.import_data_mysql.sql')
    
    # Replace placeholders
    csv_full_path = os.path.abspath(csv_file)
    create_table_sql = replace_sql_placeholders(create_table_sql, db_schema_name, table_name, csv_full_path)
    import_data_sql = replace_sql_placeholders(import_data_sql, db_schema_name, table_name, csv_full_path)
    
    # Connect to database
    connection = pymysql.connect(
        host=db_config['DB_HOST'],
        port=int(db_config['DB_PORT']),
        user=db_config['DB_USER'],
        password=db_config['DB_PASSWORD'],
        database=db_config['DB_NAME'],
        local_infile=True
    )
    
    try:
        with connection.cursor() as cursor:
            # Execute CREATE TABLE statements
            statements = [stmt.strip() for stmt in create_table_sql.split(';') if stmt.strip()]
            for statement in statements:
                if statement:
                    click.echo(f"Executing: {statement}...")
                    cursor.execute(statement)
            
            # Execute LOAD DATA statement
            click.echo("Importing data...")
            cursor.execute(import_data_sql)
            
            # Get row count
            cursor.execute(f"SELECT COUNT(*) FROM {db_schema_name}.{table_name}")
            row_count = cursor.fetchone()[0]
            click.echo(f"✓ Successfully imported {row_count:,} rows")
        
        connection.commit()
        
        # Execute post-import SQL files
        script_dir = os.path.dirname(os.path.abspath(__file__))
        post_import_files = find_post_import_sql_files(script_dir)
        execute_post_import_sql(connection, post_import_files, db_schema_name, table_name)
        
    finally:
        connection.close()


@click.command()
@click.option('--env_file_location', type=click.Path(),
              help='Path to .env file (auto-detected if not specified)')
@click.option('--csv_file', required=True, type=click.Path(),
              help='Path to CSV file to import')
@click.option('--db_schema_name', required=False,
              help='Database schema/database name (can be set via DB_SCHEMA env var)')
@click.option('--table_name', required=False,
              help='Table name for the imported data (can be set via DB_TABLE env var)')
@click.option('--trample', is_flag=True, default=False,
              help='Overwrite existing table data')
def main(env_file_location, csv_file, db_schema_name, table_name, trample):
    """
    Import CSV data into MySQL database using pre-generated SQL scripts.
    
    This script was generated by CSViper for the CSV file: test_data.csv
    """
    try:
        # Expand user path (handle ~ symbol)
        csv_file = os.path.expanduser(csv_file)
        
        # Validate CSV file exists
        if not os.path.exists(csv_file):
            raise FileNotFoundError(f"CSV file not found: {csv_file}")
        
        # Find .env file
        if env_file_location:
            env_file_location = os.path.expanduser(env_file_location)
            if not os.path.exists(env_file_location):
                raise FileNotFoundError(f".env file not found: {env_file_location}")
            env_file = env_file_location
        else:
            env_file = find_env_file()
            if not env_file:
                raise FileNotFoundError("No .env file found. Specify --env_file_location or place .env in current/parent directory")
        
        # Check .gitignore
        check_gitignore_for_env()
        
        # Load environment variables
        load_dotenv(env_file)
        
        # Get database configuration
        required_vars = ['DB_HOST', 'DB_PORT', 'DB_USER', 'DB_PASSWORD', 'DB_NAME']
        optional_vars = ['DB_SCHEMA', 'DB_TABLE', 'DEBUG', 'SECRET_KEY']
        db_config = {}
        
        for var in required_vars:
            value = os.getenv(var)
            if not value:
                raise ValueError(f"Required environment variable not found: {var}")
            db_config[var] = value
        
        for var in optional_vars:
            value = os.getenv(var)
            if value:
                db_config[var] = value
        
        # Load metadata to validate CSV header
        script_dir = os.path.dirname(os.path.abspath(__file__))
        metadata_file = os.path.join(script_dir, 'test_data.metadata.json')
        
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        
        # Determine schema and table names
        if not db_schema_name:
            db_schema_name = db_config.get('DB_SCHEMA')
            if not db_schema_name:
                raise ValueError("Database schema name must be provided via --db_schema_name or DB_SCHEMA environment variable")
        
        if not table_name:
            table_name = db_config.get('DB_TABLE')
            if not table_name:
                raise ValueError("Table name must be provided via --table_name or DB_TABLE environment variable")
        
        # Validate CSV header
        expected_columns = metadata['original_column_names']
        validate_csv_header(csv_file, expected_columns)
        
        # Check debug mode
        debug_mode = db_config.get('DEBUG', '').lower() in ('true', '1', 'yes', 'on')
        if debug_mode:
            click.echo("Debug mode enabled")
        
        click.echo(f"Importing {os.path.basename(csv_file)} into MySQL database")
        click.echo(f"Schema: {db_schema_name}, Table: {table_name}")
        
        if trample:
            click.echo("Warning: --trample flag is set. Existing table data will be overwritten.")
        
        # Execute MySQL import
        execute_mysql_import(db_config, db_schema_name, table_name, csv_file, trample)
        
        click.echo("✓ MySQL import completed successfully!")
        
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        sys.exit(1)


if __name__ == '__main__':
    main()
