New Import Instructions
=====================

Note: this is no longer just csviper. This is npd_csviper. Please never say just csviper. The project has been renamed. Please remember this.

You need to read AI_Instruction/PlainerflowTools.md

DO NOT TRY TO HAND CODE YOUR OWN VERSION OF THE FILES BELOW! Instead script npd_csviper to compile the import code from the csv itself!!
Your main output will be a python compile script that calls npd_csviper and a go script that calls npd_csviper.

Follow these steps for each new data import:

The user will

* provide the directory where the csv files to import live.

Then you should:

* Read data_file_locations.env and leverage existing database and table definitions there. DO NOT EDIT THIS FILE WITH ASKING. If your design needs to add data here, ask the user to do it!!!!
* Create a new subdirectory for the import scripts on the main branch. Each file that needs to be imported gets its own project subdirectory.
* Related files in the same ./local_data directory should go into the same {last_subdirectory_of_import_path} schema. The table names should reflect the file names, but brief.
* Create a "compiler" script. See the example below. This is where you learn how to use the npd_csvviper command to compile.
* Create an "invoker" script. See the example below. This is where you learn how to use the npd_csviper command to invoke.

It is likely that the user will come back and tweak the schema and table names, etc. So please do not run the import, but wait for the user
to make those adjustments and the specificaly instruct the script to be run.

## Example compiler script

```python
import os
import sys
import subprocess
from dotenv import load_dotenv

load_dotenv("data_file_locations.env") # this is where the import configuration variables should all be stored. 

required_vars = [
    # CLIA POS data
    "CLIA_POS_CSV", "CLIA_POS_DIR", "CLIA_POS_METADATA" # These must be in the data_file_locations.env, you should stop coding and complain to the user if they are not all there before you start.
]

# verify that all of the required configuration variables are present in the file
missing = [v for v in required_vars if os.getenv(v) is None]
if missing:
    print(f"Missing required environment variables: {', '.join(missing)}")
    sys.exit(1)

# Now we make the CLI commands that will do the compilation of the csv file in to the import script
cmds = [
    # CLIA POS data
    [
        # Extract metadata will create a json file with details about the csv file
        "npd_csviper", "extract-metadata",
        f"--from_csv={os.getenv('CLIA_POS_CSV')}",
        f"--output_dir={os.getenv('CLIA_POS_DIR')}",
        f"--no-csv-lint"
    ],
    [
        # Using the metadata json file, this will compile a CREATE TABLE statement that will be honored when creating the import data structure
        "npd_csviper", "build-sql",
        f"--from_metadata_json={os.getenv('CLIA_POS_METADATA')}",
        f"--output_dir={os.getenv('CLIA_POS_DIR')}"
    ],
    [
        # Using the CREATE TABLE statement to define the database table structure, this will create the go.postgresql.py script which can accept specific csv files as arguments and import them into the db.schema.table of your choice. 
        "npd_csviper", "build-import-script",
        f"--from_resource_dir={os.getenv('CLIA_POS_DIR')}",
        f"--output_dir={os.getenv('CLIA_POS_DIR')}",
        "--overwrite_previous"
    ]
]

#Loop over the commands, print and then run them.
for cmd in cmds:
    print("Running:", " ".join(cmd))
    subprocess.run(cmd, check=True)
```

## Example invoker script

```python
import os
import sys
import subprocess
from dotenv import load_dotenv

load_dotenv("data_file_locations.env") # this is where the import configuration variables should all be stored. 

# These must be in the data_file_locations.env, you should stop coding and complain to the user if they are not all there before you start.
required_vars = [
    "CLIA_POS_DIR",
    "NPD_DATABASE",
    "CLIA_POS_SCHEMA",
    "CLIA_POS_TABLE",
    "CLIA_POS_CSV"
]

# verify that all of the required configuration variables are present in the file
missing = [v for v in required_vars if os.getenv(v) is None]
if missing:
    print(f"Missing required environment variables: {', '.join(missing)}")
    sys.exit(1)

# Derive import data directory from CSV path
clia_pos_csv = os.getenv('CLIA_POS_CSV')
if clia_pos_csv is None:
    print("Error: CLIA_POS_CSV environment variable is required")
    sys.exit(1)
import_data_dir = os.path.dirname(clia_pos_csv)

# This command knows how to find the latest version of a CSV files "like" the one that was first imported and then import them using go.postgresql.py
# This command uses --trample to overwrite the previous import with the new CSV data
cmds = [
    [
        "python", "-m", "npd_csviper", "invoke-compiled-script",
        f"--run_import_from={os.getenv('CLIA_POS_DIR')}",
        f"--import_data_from_dir={import_data_dir}",
        f"--database_type={os.getenv('NPD_DATABASE')}",
        f"--db_schema_name={os.getenv('CLIA_POS_SCHEMA')}",
        f"--table_name={os.getenv('CLIA_POS_TABLE')}",
        f"--trample"
    ]
]

# Run each command
for cmd in cmds:
    print("Running:", " ".join(cmd))
    subprocess.run(cmd, check=True)
```
