#!/usr/bin/env python3
"""
PostgreSQL Import Script Generator for CSViper

Generates standalone Python scripts (go.postgresql.py) that can import CSV data
into PostgreSQL databases using the metadata and SQL files from previous stages.
"""

import os
import json


class PostgreSQLImportScriptGenerator:
    """
    Generates standalone Python import scripts for PostgreSQL CSV data loading.
    """
    
    @staticmethod
    def fromResourceDirToScript(resource_dir, output_dir=None, overwrite_previous=False):
        """
        Generate a go.postgresql.py script from resource directory containing metadata and SQL files.
        
        Args:
            resource_dir (str): Directory containing metadata.json and SQL files
            output_dir (str): Output directory for go.postgresql.py (defaults to resource_dir)
            overwrite_previous (bool): Whether to overwrite existing go.postgresql.py file
            
        Returns:
            str: Path to the generated go.postgresql.py file
            
        Raises:
            FileNotFoundError: If required files are not found
            ValueError: If metadata is invalid
        """
        resource_dir = os.path.abspath(resource_dir)
        
        if output_dir is None:
            output_dir = resource_dir
        else:
            output_dir = os.path.abspath(output_dir)
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Find metadata file
        metadata_file = PostgreSQLImportScriptGenerator._find_metadata_file(resource_dir)
        if not metadata_file:
            raise FileNotFoundError(f"No metadata JSON file found in {resource_dir}")
        
        # Load metadata
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        
        # Validate required PostgreSQL SQL files exist
        PostgreSQLImportScriptGenerator._validate_postgresql_sql_files(resource_dir, metadata)
        
        # Generate go.postgresql.py script
        go_script_path = os.path.join(output_dir, 'go.postgresql.py')
        
        if os.path.exists(go_script_path) and not overwrite_previous:
            raise FileExistsError(f"go.postgresql.py already exists: {go_script_path}. Use overwrite_previous=True to overwrite.")
        
        script_content = PostgreSQLImportScriptGenerator._generate_postgresql_script_content(metadata)
        
        with open(go_script_path, 'w') as f:
            f.write(script_content)
        
        # Make the script executable
        os.chmod(go_script_path, 0o755)
        
        return go_script_path
    
    @staticmethod
    def _find_metadata_file(resource_dir):
        """Find the metadata JSON file in the resource directory."""
        for filename in os.listdir(resource_dir):
            if filename.endswith('.metadata.json'):
                return os.path.join(resource_dir, filename)
        return None
    
    @staticmethod
    def _validate_postgresql_sql_files(resource_dir, metadata):
        """Validate that required PostgreSQL SQL files exist."""
        csv_basename = os.path.splitext(metadata['filename'])[0]
        
        required_files = [
            f"{csv_basename}.create_table_postgres.sql",
            f"{csv_basename}.import_data_postgres.sql"
        ]
        
        missing_files = []
        for filename in required_files:
            filepath = os.path.join(resource_dir, filename)
            if not os.path.exists(filepath):
                missing_files.append(filename)
        
        if missing_files:
            raise FileNotFoundError(f"Missing required PostgreSQL SQL files: {', '.join(missing_files)}")
    
    @staticmethod
    def _generate_postgresql_script_content(metadata):
        """Generate the content of the go.postgresql.py script."""
        csv_basename = os.path.splitext(metadata['filename'])[0]
        
        script_content = f'''#!/usr/bin/env python3
"""
PostgreSQL CSV Import Script - Generated by CSViper

This script imports CSV data into a PostgreSQL database using pre-generated SQL scripts.

Original CSV: {metadata['filename']}
Generated on: {PostgreSQLImportScriptGenerator._get_timestamp()}
"""

import os
import sys
import json
import csv
import click
from dotenv import load_dotenv


def find_post_import_sql_files(script_dir):
    """
    Find and return post-import SQL files in execution order.
    
    Args:
        script_dir (str): Directory containing the script
        
    Returns:
        List[Tuple[int, str]]: List of (order, filepath) tuples sorted by order
    """
    import glob
    
    post_import_dir = os.path.join(script_dir, 'post_import_sql')
    if not os.path.exists(post_import_dir):
        return []
    
    files_with_order = []
    
    # Look for SQL files in subdirectories
    for root, dirs, files in os.walk(post_import_dir):
        for filename in files:
            # First, look for PostgreSQL-specific files
            if filename.endswith('.postgresql.sql'):
                # Extract numeric prefix
                try:
                    order_str = filename.split('_')[0]
                    order = int(order_str)
                    filepath = os.path.join(root, filename)
                    files_with_order.append((order, filepath))
                except (ValueError, IndexError):
                    # Skip files that don't follow the naming convention
                    continue
    
    # If no PostgreSQL-specific files found, look for generic .sql files
    if not files_with_order:
        for root, dirs, files in os.walk(post_import_dir):
            for filename in files:
                if filename.endswith('.sql') and not filename.endswith('.mysql.sql'):
                    # Extract numeric prefix
                    try:
                        order_str = filename.split('_')[0]
                        order = int(order_str)
                        filepath = os.path.join(root, filename)
                        files_with_order.append((order, filepath))
                    except (ValueError, IndexError):
                        # Skip files that don't follow the naming convention
                        continue
    
    # Sort by order
    files_with_order.sort(key=lambda x: x[0])
    
    return files_with_order


def execute_post_import_sql(connection, post_import_files, db_schema_name, table_name):
    """
    Execute post-import SQL files in order.
    
    Args:
        connection: Database connection
        post_import_files (List[Tuple[int, str]]): List of (order, filepath) tuples
        db_schema_name (str): Database schema name
        table_name (str): Table name
    """
    if not post_import_files:
        click.echo("No post-import SQL files found")
        return
    
    click.echo(f"Executing {{len(post_import_files)}} post-import SQL files...")
    
    with connection.cursor() as cursor:
        for order, filepath in post_import_files:
            filename = os.path.basename(filepath)
            click.echo(f"Executing post-import SQL: {{filename}}")
            
            # Read SQL file
            with open(filepath, 'r', encoding='utf-8') as f:
                sql_content = f.read()
            
            # Replace placeholders
            sql_content = sql_content.replace('REPLACE_ME_DATABASE_NAME', db_schema_name) \\
                                   .replace('REPLACE_ME_TABLE_NAME', table_name)
            
            # Split into individual statements and execute
            statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]
            for statement in statements:
                if statement and not statement.startswith('--'):
                    try:
                        click.echo(f"  Executing: {{statement}}")
                        cursor.execute(statement)
                        connection.commit()
                    except Exception as e:
                        click.echo(f"Warning: Error executing statement in {{filename}}: {{e}}")
                        click.echo(f"  Failed statement: {{statement}}")
                        # Continue with next statement
                        continue
    
    click.echo("✓ Post-import SQL execution completed")


def find_env_file():
    """
    Find .env file in current directory or parent directory.
    
    Returns:
        str: Path to .env file or None if not found
    """
    # Check current directory
    current_dir_env = os.path.join(os.getcwd(), '.env')
    if os.path.exists(current_dir_env):
        return current_dir_env
    
    # Check parent directory
    parent_dir_env = os.path.join(os.path.dirname(os.getcwd()), '.env')
    if os.path.exists(parent_dir_env):
        return parent_dir_env
    
    return None


def check_gitignore_for_env():
    """
    Check if .env is excluded in local .gitignore file.
    Warns if .env is not properly excluded.
    """
    gitignore_path = os.path.join(os.getcwd(), '.gitignore')
    if os.path.exists(gitignore_path):
        with open(gitignore_path, 'r') as f:
            gitignore_content = f.read()
            if '.env' not in gitignore_content:
                click.echo("Warning: .env file should be added to .gitignore to avoid committing credentials")
    else:
        click.echo("Warning: No .gitignore file found. Consider creating one and adding .env to it")


def validate_csv_header(csv_file, expected_columns):
    """
    Validate that CSV header matches expected columns from metadata.
    
    Args:
        csv_file (str): Path to CSV file
        expected_columns (list): Expected column names from metadata
        
    Raises:
        ValueError: If headers don't match
    """
    with open(csv_file, 'r', newline='') as f:
        reader = csv.reader(f)
        actual_header = next(reader)
    
    if len(actual_header) != len(expected_columns):
        raise ValueError(f"Column count mismatch: Expected {{len(expected_columns)}}, got {{len(actual_header)}}")
    
    for i, (expected, actual) in enumerate(zip(expected_columns, actual_header)):
        if expected != actual:
            raise ValueError(f"Column {{i+1}} mismatch: Expected '{{expected}}', got '{{actual}}'")


def load_sql_file(filename):
    """
    Load SQL content from file.
    
    Args:
        filename (str): SQL filename
        
    Returns:
        str: SQL content
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    sql_path = os.path.join(script_dir, filename)
    
    if not os.path.exists(sql_path):
        raise FileNotFoundError(f"SQL file not found: {{sql_path}}")
    
    with open(sql_path, 'r') as f:
        return f.read()


def replace_sql_placeholders(sql_content, db_name, table_name, csv_path):
    """
    Replace placeholders in SQL content with actual values.
    
    Args:
        sql_content (str): SQL content with placeholders
        db_name (str): Database/schema name
        table_name (str): Table name
        csv_path (str): Full path to CSV file
        
    Returns:
        str: SQL content with placeholders replaced
    """
    return sql_content.replace('REPLACE_ME_DB_NAME', db_name) \\
                     .replace('REPLACE_ME_TABLE_NAME', table_name) \\
                     .replace('REPLACE_ME_CSV_FULL_PATH', csv_path)


def execute_postgresql_import(db_config, db_schema_name, table_name, csv_file, trample):
    """
    Execute PostgreSQL import process.
    
    Args:
        db_config (dict): Database configuration
        db_schema_name (str): Database schema name
        table_name (str): Table name
        csv_file (str): Path to CSV file
        trample (bool): Whether to overwrite existing data
    """
    try:
        import psycopg2
        import psycopg2.extras
    except ImportError:
        raise ImportError("psycopg2 library is required for PostgreSQL imports. Install with: pip install psycopg2-binary")
    
    # Load SQL files
    create_table_sql = load_sql_file('{csv_basename}.create_table_postgres.sql')
    
    # Replace placeholders
    csv_full_path = os.path.abspath(csv_file)
    create_table_sql = replace_sql_placeholders(create_table_sql, db_schema_name, table_name, csv_full_path)
    
    # Connect to database
    connection = psycopg2.connect(
        host=db_config['DB_HOST'],
        port=int(db_config['DB_PORT']),
        user=db_config['DB_USER'],
        password=db_config['DB_PASSWORD'],
        database=db_config['DB_NAME']
    )
    
    try:
        with connection.cursor() as cursor:
            # Create schema if it doesn't exist
            cursor.execute(f"CREATE SCHEMA IF NOT EXISTS {{db_schema_name}}")
            
            # Execute CREATE TABLE statements
            statements = [stmt.strip() for stmt in create_table_sql.split(';') if stmt.strip()]
            for statement in statements:
                if statement:
                    click.echo(f"Executing: {{statement}}...")
                    cursor.execute(statement)
            
            # Import data using COPY FROM STDIN with progress bar
            click.echo("Importing data...")
            
            # Get file size for progress tracking
            file_size = os.path.getsize(csv_file)
            
            # Build the COPY command
            copy_sql = f"COPY {{db_schema_name}}.{{table_name}} FROM STDIN WITH CSV HEADER"
            
            # Create a progress bar wrapper for the file
            class ProgressFileWrapper:
                def __init__(self, file_obj, file_size):
                    self.file_obj = file_obj
                    self.file_size = file_size
                    self.bytes_read = 0
                    self.progress_bar = click.progressbar(length=file_size, 
                                                        label='Uploading CSV data',
                                                        show_percent=True,
                                                        show_eta=True)
                    self.progress_bar.__enter__()
                
                def read(self, size=-1):
                    data = self.file_obj.read(size)
                    if data:
                        self.bytes_read += len(data)
                        self.progress_bar.update(len(data))
                    return data
                
                def readline(self):
                    line = self.file_obj.readline()
                    if line:
                        self.bytes_read += len(line)
                        self.progress_bar.update(len(line))
                    return line
                
                def __getattr__(self, name):
                    return getattr(self.file_obj, name)
                
                def close(self):
                    self.progress_bar.__exit__(None, None, None)
                    self.file_obj.close()
            
            with open(csv_file, 'r') as f:
                progress_wrapper = ProgressFileWrapper(f, file_size)
                try:
                    cursor.copy_expert(copy_sql, progress_wrapper)
                finally:
                    progress_wrapper.close()
            
            # Get row count
            cursor.execute(f"SELECT COUNT(*) FROM {{db_schema_name}}.{{table_name}}")
            row_count = cursor.fetchone()[0]
            click.echo(f"✓ Successfully imported {{row_count:,}} rows")
        
        connection.commit()
        
        # Execute post-import SQL files
        script_dir = os.path.dirname(os.path.abspath(__file__))
        post_import_files = find_post_import_sql_files(script_dir)
        execute_post_import_sql(connection, post_import_files, db_schema_name, table_name)
        
    finally:
        connection.close()


@click.command()
@click.option('--env_file_location', type=click.Path(),
              help='Path to .env file (auto-detected if not specified)')
@click.option('--csv_file', required=True, type=click.Path(),
              help='Path to CSV file to import')
@click.option('--db_schema_name', required=False,
              help='Database schema name (can be set via DB_SCHEMA env var)')
@click.option('--table_name', required=False,
              help='Table name for the imported data (can be set via DB_TABLE env var)')
@click.option('--trample', is_flag=True, default=False,
              help='Overwrite existing table data')
def main(env_file_location, csv_file, db_schema_name, table_name, trample):
    """
    Import CSV data into PostgreSQL database using pre-generated SQL scripts.
    
    This script was generated by CSViper for the CSV file: {metadata['filename']}
    """
    try:
        # Expand user path (handle ~ symbol)
        csv_file = os.path.expanduser(csv_file)
        
        # Validate CSV file exists
        if not os.path.exists(csv_file):
            raise FileNotFoundError(f"CSV file not found: {{csv_file}}")
        
        # Find .env file
        if env_file_location:
            env_file_location = os.path.expanduser(env_file_location)
            if not os.path.exists(env_file_location):
                raise FileNotFoundError(f".env file not found: {{env_file_location}}")
            env_file = env_file_location
        else:
            env_file = find_env_file()
            if not env_file:
                raise FileNotFoundError("No .env file found. Specify --env_file_location or place .env in current/parent directory")
        
        # Check .gitignore
        check_gitignore_for_env()
        
        # Load environment variables
        load_dotenv(env_file)
        
        # Get database configuration
        required_vars = ['DB_HOST', 'DB_PORT', 'DB_USER', 'DB_PASSWORD', 'DB_NAME']
        optional_vars = ['DB_SCHEMA', 'DB_TABLE', 'DEBUG', 'SECRET_KEY']
        db_config = {{}}
        
        for var in required_vars:
            value = os.getenv(var)
            if not value:
                raise ValueError(f"Required environment variable not found: {{var}}")
            db_config[var] = value
        
        for var in optional_vars:
            value = os.getenv(var)
            if value:
                db_config[var] = value
        
        # Load metadata to validate CSV header
        script_dir = os.path.dirname(os.path.abspath(__file__))
        metadata_file = os.path.join(script_dir, '{csv_basename}.metadata.json')
        
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        
        # Determine schema and table names
        if not db_schema_name:
            db_schema_name = db_config.get('DB_SCHEMA')
            if not db_schema_name:
                raise ValueError("Database schema name must be provided via --db_schema_name or DB_SCHEMA environment variable")
        
        if not table_name:
            table_name = db_config.get('DB_TABLE')
            if not table_name:
                raise ValueError("Table name must be provided via --table_name or DB_TABLE environment variable")
        
        # Validate CSV header
        expected_columns = metadata['original_column_names']
        validate_csv_header(csv_file, expected_columns)
        
        # Check debug mode
        debug_mode = db_config.get('DEBUG', '').lower() in ('true', '1', 'yes', 'on')
        if debug_mode:
            click.echo("Debug mode enabled")
        
        click.echo(f"Importing {{os.path.basename(csv_file)}} into PostgreSQL database")
        click.echo(f"Schema: {{db_schema_name}}, Table: {{table_name}}")
        
        if trample:
            click.echo("Warning: --trample flag is set. Existing table data will be overwritten.")
        
        # Execute PostgreSQL import
        execute_postgresql_import(db_config, db_schema_name, table_name, csv_file, trample)
        
        click.echo("✓ PostgreSQL import completed successfully!")
        
    except Exception as e:
        click.echo(f"Error: {{e}}", err=True)
        sys.exit(1)


if __name__ == '__main__':
    main()
'''
        
        return script_content
    
    @staticmethod
    def _get_timestamp():
        """Get current timestamp for script generation."""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
